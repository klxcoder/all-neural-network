# Number of neurons in each layer
- (layer 0) has 4 neurons = input layer
- (layer 1) has 3 neurons = 1st hidden layer 
- (layer 2) has 2 neurons = output layer

# Shapes of $x_i$ and $a_i$ 
- Feed 2 rows to the network
- x0, a0 has shape (2, 4) 
- x1, a1 has shape (2, 3)
- x2, a2 has shape (2, 2)

# Shapes of $w_i$ and $b_i$
- w0, b0 has shape (4, 3)
- w1, b1 has shape (3, 2)

# Activation functions use in each layer
- layer 0: a = x
- layer 1: a = ReLU(x)
- Layer 2: a = Softmax(x)